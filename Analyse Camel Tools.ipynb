{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the data\n",
    "import pandas as pd\n",
    "stories=pd.DataFrame()\n",
    "comments=pd.DataFrame()\n",
    "topics=[\"tamazight\",\"sport\",\"societe\",\"regions\",\"politique\",\"orbites\",\"medias\",\"marocains-du-monde\",\"faits-divers\",\"economie\",\"art-et-culture\"]\n",
    "for topic in topics:\n",
    "    stories=pd.concat([stories,pd.read_csv(\"stories_\"+topic+\".csv\")])\n",
    "    comments=pd.concat([comments,pd.read_csv(\"comments_\"+topic+\".csv\")])\n",
    "comments.drop(columns=[\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "stories.drop(columns=[\"Unnamed: 0\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng=pd.read_csv(\"hespress_comments_en.csv\")\n",
    "#lets sample 1000 comments\n",
    "commentsSample=eng.sample(10000,random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3844     What did the artist offer to the country and w...\n",
       "45451    We read in your pulpit a few weeks ago that th...\n",
       "15531    By God Almighty, I shed tears. I am longing fo...\n",
       "18223    La vie est le reflet de ce que tu crois et pen...\n",
       "46926    My question is for the specialists. Does the v...\n",
       "                               ...                        \n",
       "67344    The immigrant pays the high and precious price...\n",
       "32106    If everything is reduced, Messi gives everythi...\n",
       "35608    I am surprised at these nine children who left...\n",
       "29670    There is no power but from God. Oh God, this evil\n",
       "1985     To God and to Him we return to the survival of...\n",
       "Name: en, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commentsSample[\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1024 18:24:39.559741  3436 file_utils.py:39] PyTorch version 1.6.0 available.\n",
      "I1024 18:24:43.860897  3436 configuration_utils.py:262] loading configuration file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert\\config.json\n",
      "I1024 18:24:43.861899  3436 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": \"arabic_sentiment\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"positive\",\n",
      "    \"1\": \"negative\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 1,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "I1024 18:24:43.882801  3436 modeling_utils.py:665] loading weights file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert\\pytorch_model.bin\n",
      "I1024 18:24:47.599990  3436 modeling_utils.py:765] All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "I1024 18:24:47.600985  3436 modeling_utils.py:774] All the weights of BertForSequenceClassification were initialized from the model checkpoint at C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "I1024 18:24:47.601982  3436 tokenization_utils_base.py:1167] Model name 'C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I1024 18:24:47.603977  3436 tokenization_utils_base.py:1197] Didn't find file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert\\added_tokens.json. We won't load it.\n",
      "I1024 18:24:47.604975  3436 tokenization_utils_base.py:1197] Didn't find file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert\\tokenizer.json. We won't load it.\n",
      "I1024 18:24:47.605973  3436 tokenization_utils_base.py:1252] loading file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert\\vocab.txt\n",
      "I1024 18:24:47.606969  3436 tokenization_utils_base.py:1252] loading file None\n",
      "I1024 18:24:47.606969  3436 tokenization_utils_base.py:1252] loading file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert\\special_tokens_map.json\n",
      "I1024 18:24:47.607966  3436 tokenization_utils_base.py:1252] loading file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\sentiment_analysis\\arabert\\tokenizer_config.json\n",
      "I1024 18:24:47.608964  3436 tokenization_utils_base.py:1252] loading file None\n"
     ]
    }
   ],
   "source": [
    "#getting arabic sentiment\n",
    "\n",
    "from camel_tools.sentiment import SentimentAnalyzer\n",
    "sa = SentimentAnalyzer.pretrained()\n",
    "predArabic= sa.predict(commentsSample.comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commentsSample[\"positive\"] =list(map(lambda prediction: prediction[\"positive\"],predArabic))\n",
    "# commentsSample[\"negative\"] = list(map(lambda prediction: prediction[\"negative\"],predArabic))\n",
    "# commentsSample[\"neutral\"] =list(map(lambda prediction: prediction[\"neutral\"],predArabic))\n",
    "\n",
    "# commentsSample[\"positive\"] =list(map(lambda prediction: prediction[\"positive\"],predArabic))\n",
    "# commentsSample[\"negative\"] = list(map(lambda prediction: prediction[\"negative\"],predArabic))\n",
    "commentsSample[\"sentimentArabic\"]= predArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-bba6a9d0c522>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-bba6a9d0c522>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    commentsSample.\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# commentsSample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def indexToPred(index):\n",
    "    preds=[\"positive\",\"negative\",\"neutral\"]\n",
    "    return preds[index] \n",
    "\n",
    "def polariyToSentiment(polarity):\n",
    "    if polarity>0:\n",
    "        return \"positive\"\n",
    "    elif polarity==0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "# commentsSample[\"sentimentArabic\"]=commentsSample.apply(lambda comment: indexToPred(np.argmax([comment.positive,comment.negative,comment.neutral])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting english sentiment\n",
    "\n",
    "from textblob import TextBlob\n",
    "sentiments=[]\n",
    "for sentence in commentsSample.en:\n",
    "    sentiments.append(TextBlob(sentence).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentsSample[\"sentimentEnglish\"]=[polariyToSentiment(sentiment.polarity) for sentiment in sentiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3951"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting how much they resamble\n",
    "commentsSample.apply(lambda comment: comment.sentimentEnglish==comment.sentimentArabic,axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 10000 comments only 3951 are similar between Camel Tools and Text Blob using translated text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6049"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000-3951\n",
    "\n",
    "#60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.30      0.83      0.44      2336\n",
      "     neutral       0.43      0.22      0.29      2337\n",
      "    positive       0.65      0.28      0.39      5327\n",
      "\n",
      "    accuracy                           0.40     10000\n",
      "   macro avg       0.46      0.44      0.37     10000\n",
      "weighted avg       0.52      0.40      0.38     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(commentsSample.sentimentEnglish,commentsSample.sentimentArabic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment             مغادي نهضر على حتى شي حاجة عندها علاقة بان هاد الوباء عطى و بين المكانة اللي كيستحق كل واحد\\nولكن الحاجة اللي غادي نعلق عليها هي كلمة النجمين لا اظن ان كلمة نجم عدنا فالمغرب                                                                                      \n",
       "en                  I will go out, until something needs to be done. It has a relationship that the head of the epidemic is given and the status that everyone deserves\\nBut the need that we attach to is the word al-najmeen. I don’t think the word “star” is our return, so Morocco\n",
       "sentimentArabic     negative                                                                                                                                                                                                                                                           \n",
       "sentimentEnglish    neutral                                                                                                                                                                                                                                                            \n",
       "Name: 3883, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "commentsSample.iloc[5][[\"comment\",\"en\",\"sentimentArabic\",\"sentimentEnglish\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1022 08:13:09.029264 16104 file_utils.py:39] PyTorch version 1.6.0 available.\n",
      "I1022 08:13:13.295162 16104 configuration_utils.py:262] loading configuration file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert\\config.json\n",
      "I1022 08:13:13.297128 16104 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-LOC\",\n",
      "    \"1\": \"B-MISC\",\n",
      "    \"2\": \"B-ORG\",\n",
      "    \"3\": \"B-PERS\",\n",
      "    \"4\": \"I-LOC\",\n",
      "    \"5\": \"I-MISC\",\n",
      "    \"6\": \"I-ORG\",\n",
      "    \"7\": \"I-PERS\",\n",
      "    \"8\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 0,\n",
      "    \"B-MISC\": 1,\n",
      "    \"B-ORG\": 2,\n",
      "    \"B-PERS\": 3,\n",
      "    \"I-LOC\": 4,\n",
      "    \"I-MISC\": 5,\n",
      "    \"I-ORG\": 6,\n",
      "    \"I-PERS\": 7,\n",
      "    \"O\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "I1022 08:13:13.305109 16104 modeling_utils.py:665] loading weights file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert\\pytorch_model.bin\n",
      "I1022 08:13:17.296306 16104 modeling_utils.py:765] All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "I1022 08:13:17.297301 16104 modeling_utils.py:774] All the weights of BertForTokenClassification were initialized from the model checkpoint at C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "I1022 08:13:17.299297 16104 tokenization_utils_base.py:1167] Model name 'C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I1022 08:13:17.302289 16104 tokenization_utils_base.py:1197] Didn't find file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert\\added_tokens.json. We won't load it.\n",
      "I1022 08:13:17.304283 16104 tokenization_utils_base.py:1197] Didn't find file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert\\tokenizer.json. We won't load it.\n",
      "I1022 08:13:17.305280 16104 tokenization_utils_base.py:1252] loading file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert\\vocab.txt\n",
      "I1022 08:13:17.307276 16104 tokenization_utils_base.py:1252] loading file None\n",
      "I1022 08:13:17.309270 16104 tokenization_utils_base.py:1252] loading file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert\\special_tokens_map.json\n",
      "I1022 08:13:17.310267 16104 tokenization_utils_base.py:1252] loading file C:\\Users\\tariq\\AppData\\Roaming\\camel_tools\\data\\ner\\arabert\\tokenizer_config.json\n",
      "I1022 08:13:17.311265 16104 tokenization_utils_base.py:1252] loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('سمير', 'B-PERS'), ('ذهب', 'O'), ('الى', 'O'), ('مراكش', 'B-LOC'), ('لشراء', 'O'), ('هاتف', 'O'), ('سامسونغ.', 'B-ORG')]\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.ner import NERecognizer\n",
    "\n",
    "ner = NERecognizer.pretrained()\n",
    "\n",
    "# Predict the labels of a single sentence.\n",
    "# The sentence must be pretokenized by whitespace and punctuation.\n",
    "\n",
    "\n",
    "sentence = 'سمير ذهب الى مراكش لشراء هاتف سامسونغ.'.split()\n",
    "\n",
    "labels = ner.predict_sentence(sentence)\n",
    "\n",
    "# Print the list of token-label pairs\n",
    "print(list(zip(sentence, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['verb', 'prep', 'noun']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tagger.default import DefaultTagger\n",
    "\n",
    "mled = MLEDisambiguator.pretrained()\n",
    "tagger = DefaultTagger(mled, 'pos')\n",
    "\n",
    "tagger.tag('ذهبت الى المدرسة'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ذَهَبْتُ', 'إِلَى', 'المَدْرَسَةِ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = DefaultTagger(mled, 'diac')\n",
    "\n",
    "tagger.tag('ذهبت الى المدرسة'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'na', 'p']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = DefaultTagger(mled, 'num')\n",
    "tagger.tag('ذهبت الى المدارس'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m', 'm', 'na', 'f', 'f']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = DefaultTagger(mled, 'form_gen')\n",
    "tagger.tag('اكل الولد و الفتاة التفاحة'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['أَكَل-u_1', 'وَلَد_1', 'وَ_1', 'فَتاة_1', 'تُفّاح_1']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization\n",
    "\n",
    "tagger = DefaultTagger(mled, 'lex')\n",
    "tagger.tag('اكل الولد و الفتاة التفاحة'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سَوْفَ نَقْرَأ الكُتُبِ\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "\n",
    "mle = MLEDisambiguator.pretrained()\n",
    "\n",
    "# We expect a sentence to be whitespace/punctuation tokenized beforehand.\n",
    "# We provide a simple whitespace and punctuation tokenizer as part of camel_tools.\n",
    "# See camel_tools.tokenizers.word.simple_word_tokenize.\n",
    "sentence = ['سوف', 'نقرأ', 'الكتب']\n",
    "\n",
    "disambig = mle.disambiguate(sentence)\n",
    "\n",
    "# Let's, for example, use the top disambiguations to generate a diacritized\n",
    "# version of the above sentence.\n",
    "# Note that, in practice, you'll need to make sure that each word has a\n",
    "# non-zero list of analyses.\n",
    "diacritized = [d.analyses[0].analysis['diac'] for d in disambig]\n",
    "print(' '.join(diacritized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialect Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "camel_tools.dialectid is not available on Windows.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7d6b853e8ee4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcamel_tools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdialectid\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDialectIdentifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDialectIdentifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m sentences = [\n",
      "\u001b[1;32m~\\Desktop\\Alexis NLP\\camel_tools\\dialectid\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'win32'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     raise ModuleNotFoundError(\n\u001b[1;32m---> 39\u001b[1;33m         'camel_tools.dialectid is not available on Windows.')\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: camel_tools.dialectid is not available on Windows."
     ]
    }
   ],
   "source": [
    "from camel_tools.dialectid import DialectIdentifier\n",
    "\n",
    "did = DialectIdentifier.pretrained()\n",
    "\n",
    "sentences = [\n",
    "    'مال الهوى و مالي شكون اللي جابني ليك  ما كنت انايا ف حالي بلاو قلبي يانا بيك',\n",
    "    'بدي دوب قلي قلي بجنون بحبك انا مجنون ما بنسى حبك يوم'\n",
    "]\n",
    "\n",
    "predictions = did.predict(sentences)\n",
    "\n",
    "# Each prediction is a tuple containing both the top prediction and the\n",
    "# percentage score of each dialect. To get only the top prediction, we can\n",
    "# do the following:\n",
    "top_dialects = [p.top for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سَوْفَ_1 قَرَأ-a_1 كِتاب_1 أَسْعَد_1\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "\n",
    "mle = MLEDisambiguator.pretrained()\n",
    "\n",
    "# We expect a sentence to be whitespace/punctuation tokenized beforehand.\n",
    "# We provide a simple whitespace and punctuation tokenizer as part of camel_tools.\n",
    "# See camel_tools.tokenizers.word.simple_word_tokenize.\n",
    "sentence = ['سوف', 'نقرأ', 'الكتب',\"يسعدني\"]\n",
    "\n",
    "disambig = mle.disambiguate(sentence)\n",
    "\n",
    "# Let's, for example, use the top disambiguations to generate a diacritized\n",
    "# version of the above sentence.\n",
    "# Note that, in practice, you'll need to make sure that each word has a\n",
    "# non-zero list of analyses.\n",
    "diacritized = [d.analyses[0].analysis['lex'] for d in disambig]\n",
    "print(' '.join(diacritized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DisambiguatedWord(word='الكتب', analyses=[ScoredAnalysis(score=1.0, analysis={'diac': 'الكُتُبِ', 'lex': 'كِتاب_1', 'bw': 'ال/DET+كُتُب/NOUN+ِ/CASE_DEF_GEN', 'gloss': 'the+books+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'num': 'p', 'd2seg': 'الكُتُبِ', 'd1tok': 'الكُتُبِ', 'root': 'ك.ت.ب', 'd3tok': 'ال+_كُتُبِ', 'caphi': '2_a_l_k_u_t_u_b_i', 'pos_lex_logprob': -3.511249, 'd1seg': 'الكُتُبِ', 'ud': 'DET+NOUN', 'd2tok': 'الكُتُبِ', 'lex_logprob': -3.511249, 'atbtok': 'الكُتُبِ', 'gen': 'm', 'd3seg': 'ال+_كُتُبِ', 'catib6': 'PRT+NOM', 'atbseg': 'الكُتُبِ', 'pattern': 'ال1ُ2ُ3ِ', 'pos_logprob': -0.4344233, 'stem': 'كُتُب', 'stemgloss': 'books', 'stemcat': 'N'})])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
